# fly.toml app configuration file generated for dogexbt-crawler on 2024-12-30T16:25:13+01:00
#
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.
#

app = 'dogeai-agent'
primary_region = 'bos'

[processes]
agent = "node ./server.js"
chat = "node ./chat-server.js"

[build]
dockerfile = "./Dockerfile"

[http_service]
processes = ["agent"]
internal_port = 3000
force_https = true
auto_stop_machines = 'off'
auto_start_machines = true
min_machines_running = 1

[[http_service.checks]]
grace_period = "10s"
interval = "30s"
method = "GET"
timeout = "5s"
path = "/api/health"

[[metrics]]
port = 3000
path = "/api/metrics"
processes = ["agent"]


# LLM agent service (chat) on llm.dogeai-agent.fly.dev:8080 (port 8080, HTTP)
[[services]]
internal_port = 3001
protocol = "tcp"
processes = ["chat"]
auto_stop_machines = 'off'
auto_start_machines = true
min_machines_running = 1

[[services.ports]]
port = 8080
handlers = ["http"]

[[services.checks]]
grace_period = "10s"
interval = "30s"
method = "GET"
timeout = "5s"
path = "/api/health"

# Metrics for LLM agent (chat)
[[metrics]]
port = 3001
path = "/api/metrics"
processes = ["chat"]

# VM configuration for HTTP API (agent)
[[vm]]
memory = '512mb'
cpu_kind = 'shared'
cpus = 2
processes = ["agent"]

# VM configuration for LLM agent (chat)
[[vm]]
memory = '512mb'
cpu_kind = 'shared'
cpus = 1
processes = ["chat"]
