# fly.toml app configuration file
# See https://fly.io/docs/reference/configuration/ for information about how to use this file.

app = 'dogeai-agent'
primary_region = 'bos'

[processes]
agent = "node ./server.js"
chat = "node ./chat-server.js"

[build]
dockerfile = "./Dockerfile"

[http_service]
processes = ["agent"]
internal_port = 3000
force_https = true
auto_stop_machines = 'off'
auto_start_machines = true
min_machines_running = 1

[[http_service.checks]]
grace_period = "10s"
interval = "30s"
method = "GET"
timeout = "5s"
path = "/api/health"

[[metrics]]
port = 3000
path = "/api/metrics"
processes = ["agent"]

# Chat service configuration
[[services]]
internal_port = 3001
processes = ["chat"]
protocol = "tcp"
auto_stop_machines = 'off'
auto_start_machines = true
min_machines_running = 1

[[services.ports]]
port = 3001
handlers = ["http"]
force_https = false # Use CF for SSL

[[services.http_checks]]
interval = "30s"
grace_period = "10s"
method = "GET"
path = "/api/health"
protocol = "http"
timeout = "5s"

# Metrics for LLM agent (chat)
[[metrics]]
processes = ["chat"]
port = 3001
path = "/api/metrics"

# VM configuration for HTTP API (agent)
[[vm]]
memory = '512mb'
cpu_kind = 'shared'
cpus = 2
processes = ["agent"]

# VM configuration for LLM agent (chat)
[[vm]]
memory = '512mb'
cpu_kind = 'shared'
cpus = 1
processes = ["chat"]
